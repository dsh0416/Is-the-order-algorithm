# 合适的工具
## 什么样的算法是好的？
### 以数据规模为尺度的性能分析
~~闷声发大财是最好的。~~ 我们先来考虑一下如何评价一个算法的好坏。之前我们说到算法常遇到的问题是运算时间过长和占用空间（内存、硬盘）过大。由于涉及硬盘存储的算法比较少，通常算法运行的主要瓶颈是时间和内存占用。衡量时间的尺子我们平时都在用，那就是秒表。我们只要掐个表就能知道。比如这个算法跑了 5 秒跑完了，而另一个跑 1 秒就跑完了。那么后面一种算法比前面一种会更快一点。同样，内存占用也是比较容易计算的，比如这个算法运行时程序会占用 50MB 的内存，而另一个占用了 200MB 的内存，显然后面一种算法的空间占用更大一些。

但这样的方法是不是完美的呢？实际上不是那么显然。比如我们的算法在两个不同的电脑上跑、在不同操作系统上跑、通过不同的语言写、使用不同的编译器的程序进行编译，这样的时间和内存占用都会有所区别。显然我们的运算时间和占用的内存空间上会有区别。如果简单用时间和空间占用来判断，并不能判断出“算法”的好坏，而是算法、编译、电脑性能的综合好坏。通常我们需要单独把算法拿出来评估，才能控制好变量而不受别的因素影响。

我们以时间为例，我们排除一些次要因素，通常来说，电脑的性能对你的运行时间会造成一个线性的影响。比如一个程序在一台电脑上运行需要 10 秒，另一个程序需要 20 秒。换一台电脑的情况下，第一个程序如果跑了 5 秒，另一个程序通常也能在 10 秒左右跑完，区别不会太大（在某个合理范围下）。因为通常这个运算时间是和你每秒执行了多少次运算成正相关的，而电脑的快慢一般指的就是每秒最多能执行多少次运算。所以他们的变化几乎是一个线性关系。

那么去评估这个算法时间上的好坏似乎有了一个方法，我们只要去讨论你算法一共执行了多少次运算就行了。一个执行了 1000 次运算的算法会比一个执行了 100000 次运算的算法会更 “快” 一点。不过这样的方法似乎也有些问题。比如我在 1+2+3+4+5+...+n 这个问题上时。当 n=2 时，我其实只要执行 1+2 就行了，只执行了一次运算，而这时去执行 (首项+末项)×项数÷2 却竟然执行了 4 次运算！似乎还不太合算。然而当 n=1001 时，前一种算法就要运算 1000 次了，而第二种算法还是只要运算 4 次。这时候他们的性能差距才会被拉开。

这时候我们发现，算法的性能其实通常还和要处理的数据规模有关。一些算法在处理一些小数据的时候更有优势，而另一些算法在处理大数据的时候更有优势。如果这个数据的大小我们用 n 来表示。

第一个算法的运算次数其实可以表示为 `T(n)=n-1`，而第二个算法的运算次数为 `T(n)=3`（加一次，乘一次，除一次），当 n<4 时，第一种算法更快，当 n=4 时，两种算法的运算次数是一样的，而当 n>4 时，第二种算法更快。通过这种方法我们就成功的用数据规模为尺度实现了对算法时间上的性能分析，通常空间上我们也是这么分析的。
### 大O符号的简单理解

上面的性能分析方法其实并不存在明显的漏洞，但是有时并不是很直观。由于我们的算法有时比较复杂，这个函数通常会变成 `T(n)= 6×n^5 + 300×n^2 + 1000×n + 400` 这样一长串的东西了。这时候看起来确实是挺累的，首先这里面有很多项，每一项还有不同的系数，算起来也比较累。

不过我们仔细思考一下，或许我们不一定要完全这么分析。处理的时间一般都是会随着处理的数据大小增长，数据越大处理时间越慢，只是有时这种增长比较缓慢，而有时却非常快。对于一些小数据而言，无论是比较高效的算法还是比较朴素的算法，其实区别并不大。0.01秒 跑完和 0.03秒 跑完并不会有什么区别。常常对我们造成挑战的，往往是比较庞大的数据。如果一个`T(n)`的分析最后得到的是一个多项式，通常直接影响性能的是那个次数最大的那一项，甚至这一项的系数的影响也不大，通常是那个项的次数起到一个决定因素。

于是人们发明了一个神奇的符号`大O符号`，这个符号可以用于描述一个函数的参数趋向于无穷大时函数的渐进上限，对于我们分析算法非常方便。比如上面这个`T(n)`函数，我们可以直接写`O(n^5)`，忽略所有次数小的项，同时忽略最大项的系数，得到一个虽然不那么精确，但确足够用于分析算法复杂性，而且非常好用的式子。

这个符号的详细数学定义比较复杂，但是对于初学而言并不需要这样理解。通常也并不需要进行非常严格的计算，通常计算结果会有下面这些情况（按运算时间由小到大排序）

```
O(1) 常数级别，无论输入数据多大，运算时间都不受影响。
O(logn) 对数。运算时间随着数据大小成一个对数增长。
O(n) 线性。运算时间随数据大小线性增长
O(nlogn) 上面两种相乘（注：计算机中的log通常以2为底而不是10）
O(n^2) 平方。
O(n^c)(c>2) 随着c的增加，运算时间逐渐变大，但c必须是常数。
```

上面这几种以及他们互相相乘得到的结果在时间复杂度上一般被叫做 “多项式时间”。多项式时间内可解的算法通常都可以被看作比较好的（常数不算太大的情况下），尤其是和下面这几种比起来的话。

```
O(n!) 阶乘，n大了之后很可怕。
O(c^n) 指数级增长，更可怕。
O(n^n) 写出这样的算法可能会使你有生之年找不出解。
```

有了这样的感性的认识，我想你已经对算法的复杂度有一些理解了。我们也可以继续我们对算法设计的讨论啦！

下面给出一系列的例子，你可以用大O符号来估计一下他们的`时间复杂度`来加深一下你的理解。

1. 求1到n的和，方法是将n个数字依次相加
2. 求1到n的和，方法是将n个数字首项末项相加乘项数，除以2来求他们的和
3. 求n的3次方，方法是n×n×n算出结果
4. 求n的3次方，方法是将n个n加起来，算出n的平方，再把n个n^2加起来，算出n的立方
5. 求n的n次方，方法是n个n乘起来
6. 求n的n次方，方法是把n个n相加求出n^2，再把n个n^2加起来算出n^3。。。直到求出n^n

答案：

1. O(n)
2. O(1)
3. O(1)
4. O(n)
5. O(n)
6. O(n^2)